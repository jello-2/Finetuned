{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.91968,
  "eval_steps": 500,
  "global_step": 3000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0064,
      "grad_norm": 0.9077288508415222,
      "learning_rate": 1.9148936170212766e-05,
      "loss": 0.9062,
      "step": 10
    },
    {
      "epoch": 0.0128,
      "grad_norm": 0.5913906693458557,
      "learning_rate": 4.0425531914893614e-05,
      "loss": 0.9224,
      "step": 20
    },
    {
      "epoch": 0.0192,
      "grad_norm": 0.4473552405834198,
      "learning_rate": 6.170212765957447e-05,
      "loss": 0.7743,
      "step": 30
    },
    {
      "epoch": 0.0256,
      "grad_norm": 0.42683297395706177,
      "learning_rate": 8.297872340425533e-05,
      "loss": 0.7139,
      "step": 40
    },
    {
      "epoch": 0.032,
      "grad_norm": 0.6454727649688721,
      "learning_rate": 0.00010425531914893618,
      "loss": 0.8038,
      "step": 50
    },
    {
      "epoch": 0.0384,
      "grad_norm": 0.5616289973258972,
      "learning_rate": 0.00012553191489361702,
      "loss": 0.7227,
      "step": 60
    },
    {
      "epoch": 0.0448,
      "grad_norm": 0.5503587126731873,
      "learning_rate": 0.00014680851063829788,
      "loss": 0.7108,
      "step": 70
    },
    {
      "epoch": 0.0512,
      "grad_norm": 0.5516259074211121,
      "learning_rate": 0.00016808510638297873,
      "loss": 0.765,
      "step": 80
    },
    {
      "epoch": 0.0576,
      "grad_norm": 0.9033141136169434,
      "learning_rate": 0.00018936170212765957,
      "loss": 0.7333,
      "step": 90
    },
    {
      "epoch": 0.064,
      "grad_norm": 0.43682119250297546,
      "learning_rate": 0.00019999865800655937,
      "loss": 0.7405,
      "step": 100
    },
    {
      "epoch": 0.0704,
      "grad_norm": 0.43179941177368164,
      "learning_rate": 0.0001999879222751468,
      "loss": 0.7476,
      "step": 110
    },
    {
      "epoch": 0.0768,
      "grad_norm": 0.42492374777793884,
      "learning_rate": 0.00019996645196489646,
      "loss": 0.7328,
      "step": 120
    },
    {
      "epoch": 0.0832,
      "grad_norm": 0.4059787690639496,
      "learning_rate": 0.00019993424938083404,
      "loss": 0.7471,
      "step": 130
    },
    {
      "epoch": 0.0896,
      "grad_norm": 0.589686930179596,
      "learning_rate": 0.00019989131798018897,
      "loss": 0.7006,
      "step": 140
    },
    {
      "epoch": 0.096,
      "grad_norm": 0.41875317692756653,
      "learning_rate": 0.00019983766237202292,
      "loss": 0.6593,
      "step": 150
    },
    {
      "epoch": 0.1024,
      "grad_norm": 0.5058918595314026,
      "learning_rate": 0.00019977328831673516,
      "loss": 0.764,
      "step": 160
    },
    {
      "epoch": 0.1088,
      "grad_norm": 0.3309604525566101,
      "learning_rate": 0.00019969820272544415,
      "loss": 0.6733,
      "step": 170
    },
    {
      "epoch": 0.1152,
      "grad_norm": 0.47039997577667236,
      "learning_rate": 0.00019961241365924545,
      "loss": 0.6771,
      "step": 180
    },
    {
      "epoch": 0.1216,
      "grad_norm": 0.4903656542301178,
      "learning_rate": 0.00019951593032834636,
      "loss": 0.6326,
      "step": 190
    },
    {
      "epoch": 0.128,
      "grad_norm": 0.5176934003829956,
      "learning_rate": 0.0001994087630910772,
      "loss": 0.686,
      "step": 200
    },
    {
      "epoch": 0.1344,
      "grad_norm": 0.5804599523544312,
      "learning_rate": 0.00019929092345277917,
      "loss": 0.6897,
      "step": 210
    },
    {
      "epoch": 0.1408,
      "grad_norm": 0.5498143434524536,
      "learning_rate": 0.00019916242406456898,
      "loss": 0.6668,
      "step": 220
    },
    {
      "epoch": 0.1472,
      "grad_norm": 0.43843650817871094,
      "learning_rate": 0.00019902327872198106,
      "loss": 0.7154,
      "step": 230
    },
    {
      "epoch": 0.1536,
      "grad_norm": 0.4854792058467865,
      "learning_rate": 0.00019887350236348612,
      "loss": 0.6501,
      "step": 240
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.4967409074306488,
      "learning_rate": 0.00019871311106888748,
      "loss": 0.6994,
      "step": 250
    },
    {
      "epoch": 0.1664,
      "grad_norm": 0.5187875032424927,
      "learning_rate": 0.00019854212205759484,
      "loss": 0.6535,
      "step": 260
    },
    {
      "epoch": 0.1728,
      "grad_norm": 0.4484732151031494,
      "learning_rate": 0.00019836055368677556,
      "loss": 0.6609,
      "step": 270
    },
    {
      "epoch": 0.1792,
      "grad_norm": 0.4160817861557007,
      "learning_rate": 0.00019816842544938384,
      "loss": 0.6614,
      "step": 280
    },
    {
      "epoch": 0.1856,
      "grad_norm": 0.5401455760002136,
      "learning_rate": 0.00019796575797206804,
      "loss": 0.6754,
      "step": 290
    },
    {
      "epoch": 0.192,
      "grad_norm": 0.47687143087387085,
      "learning_rate": 0.00019775257301295616,
      "loss": 0.7124,
      "step": 300
    },
    {
      "epoch": 0.1984,
      "grad_norm": 0.4918792247772217,
      "learning_rate": 0.00019752889345931997,
      "loss": 0.6576,
      "step": 310
    },
    {
      "epoch": 0.2048,
      "grad_norm": 0.4577641487121582,
      "learning_rate": 0.00019729474332511783,
      "loss": 0.6911,
      "step": 320
    },
    {
      "epoch": 0.2112,
      "grad_norm": 0.4812902808189392,
      "learning_rate": 0.00019705014774841663,
      "loss": 0.7019,
      "step": 330
    },
    {
      "epoch": 0.2176,
      "grad_norm": 0.3977544605731964,
      "learning_rate": 0.0001967951329886929,
      "loss": 0.7476,
      "step": 340
    },
    {
      "epoch": 0.224,
      "grad_norm": 0.5064756870269775,
      "learning_rate": 0.00019652972642401377,
      "loss": 0.6749,
      "step": 350
    },
    {
      "epoch": 0.2304,
      "grad_norm": 0.43803685903549194,
      "learning_rate": 0.0001962539565480975,
      "loss": 0.6465,
      "step": 360
    },
    {
      "epoch": 0.2368,
      "grad_norm": 0.4322547912597656,
      "learning_rate": 0.00019596785296725462,
      "loss": 0.6986,
      "step": 370
    },
    {
      "epoch": 0.2432,
      "grad_norm": 0.5429463982582092,
      "learning_rate": 0.00019567144639720936,
      "loss": 0.7075,
      "step": 380
    },
    {
      "epoch": 0.2496,
      "grad_norm": 0.529918909072876,
      "learning_rate": 0.00019536476865980197,
      "loss": 0.6632,
      "step": 390
    },
    {
      "epoch": 0.256,
      "grad_norm": 0.361894816160202,
      "learning_rate": 0.00019504785267957252,
      "loss": 0.6812,
      "step": 400
    },
    {
      "epoch": 0.2624,
      "grad_norm": 0.3997437059879303,
      "learning_rate": 0.00019472073248022603,
      "loss": 0.6967,
      "step": 410
    },
    {
      "epoch": 0.2688,
      "grad_norm": 0.4821859300136566,
      "learning_rate": 0.0001943834431809798,
      "loss": 0.6425,
      "step": 420
    },
    {
      "epoch": 0.2752,
      "grad_norm": 0.410457581281662,
      "learning_rate": 0.00019403602099279306,
      "loss": 0.6697,
      "step": 430
    },
    {
      "epoch": 0.2816,
      "grad_norm": 0.7196122407913208,
      "learning_rate": 0.00019367850321447928,
      "loss": 0.7138,
      "step": 440
    },
    {
      "epoch": 0.288,
      "grad_norm": 0.6571744680404663,
      "learning_rate": 0.00019331092822870205,
      "loss": 0.6941,
      "step": 450
    },
    {
      "epoch": 0.2944,
      "grad_norm": 0.4901288151741028,
      "learning_rate": 0.00019293333549785414,
      "loss": 0.6513,
      "step": 460
    },
    {
      "epoch": 0.3008,
      "grad_norm": 0.5075604319572449,
      "learning_rate": 0.00019254576555982096,
      "loss": 0.6885,
      "step": 470
    },
    {
      "epoch": 0.3072,
      "grad_norm": 0.510265588760376,
      "learning_rate": 0.00019214826002362853,
      "loss": 0.6097,
      "step": 480
    },
    {
      "epoch": 0.3136,
      "grad_norm": 0.5413945913314819,
      "learning_rate": 0.00019174086156497624,
      "loss": 0.6714,
      "step": 490
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.4503318965435028,
      "learning_rate": 0.00019132361392165534,
      "loss": 0.7159,
      "step": 500
    },
    {
      "epoch": 0.3264,
      "grad_norm": 0.5133090019226074,
      "learning_rate": 0.0001908965618888533,
      "loss": 0.6973,
      "step": 510
    },
    {
      "epoch": 0.3328,
      "grad_norm": 0.4944112300872803,
      "learning_rate": 0.00019045975131434467,
      "loss": 0.636,
      "step": 520
    },
    {
      "epoch": 0.3392,
      "grad_norm": 0.46554672718048096,
      "learning_rate": 0.00019001322909356877,
      "loss": 0.679,
      "step": 530
    },
    {
      "epoch": 0.3456,
      "grad_norm": 0.4804646968841553,
      "learning_rate": 0.0001895570431645953,
      "loss": 0.6693,
      "step": 540
    },
    {
      "epoch": 0.352,
      "grad_norm": 0.5356598496437073,
      "learning_rate": 0.00018909124250297761,
      "loss": 0.693,
      "step": 550
    },
    {
      "epoch": 0.3584,
      "grad_norm": 0.3446069657802582,
      "learning_rate": 0.00018861587711649467,
      "loss": 0.7434,
      "step": 560
    },
    {
      "epoch": 0.3648,
      "grad_norm": 0.46672523021698,
      "learning_rate": 0.00018813099803978252,
      "loss": 0.6282,
      "step": 570
    },
    {
      "epoch": 0.3712,
      "grad_norm": 0.40439075231552124,
      "learning_rate": 0.0001876366573288551,
      "loss": 0.635,
      "step": 580
    },
    {
      "epoch": 0.3776,
      "grad_norm": 0.5332492589950562,
      "learning_rate": 0.0001871329080555156,
      "loss": 0.6915,
      "step": 590
    },
    {
      "epoch": 0.384,
      "grad_norm": 0.6687415242195129,
      "learning_rate": 0.00018661980430165884,
      "loss": 0.7175,
      "step": 600
    },
    {
      "epoch": 0.3904,
      "grad_norm": 0.4215444326400757,
      "learning_rate": 0.0001860974011534649,
      "loss": 0.7321,
      "step": 610
    },
    {
      "epoch": 0.3968,
      "grad_norm": 0.5741040110588074,
      "learning_rate": 0.0001855657546954853,
      "loss": 0.714,
      "step": 620
    },
    {
      "epoch": 0.4032,
      "grad_norm": 0.4985371530056,
      "learning_rate": 0.00018502492200462182,
      "loss": 0.6762,
      "step": 630
    },
    {
      "epoch": 0.4096,
      "grad_norm": 0.40685132145881653,
      "learning_rate": 0.00018447496114399876,
      "loss": 0.6574,
      "step": 640
    },
    {
      "epoch": 0.416,
      "grad_norm": 0.44407591223716736,
      "learning_rate": 0.00018391593115672932,
      "loss": 0.7344,
      "step": 650
    },
    {
      "epoch": 0.4224,
      "grad_norm": 0.4713823199272156,
      "learning_rate": 0.0001833478920595769,
      "loss": 0.6627,
      "step": 660
    },
    {
      "epoch": 0.4288,
      "grad_norm": 0.4878111183643341,
      "learning_rate": 0.00018277090483651164,
      "loss": 0.6575,
      "step": 670
    },
    {
      "epoch": 0.4352,
      "grad_norm": 0.3324064314365387,
      "learning_rate": 0.00018218503143216345,
      "loss": 0.6476,
      "step": 680
    },
    {
      "epoch": 0.4416,
      "grad_norm": 0.4455634653568268,
      "learning_rate": 0.0001815903347451715,
      "loss": 0.6966,
      "step": 690
    },
    {
      "epoch": 0.448,
      "grad_norm": 0.44664469361305237,
      "learning_rate": 0.0001809868786214316,
      "loss": 0.7293,
      "step": 700
    },
    {
      "epoch": 0.4544,
      "grad_norm": 0.3552851676940918,
      "learning_rate": 0.0001803747278472419,
      "loss": 0.688,
      "step": 710
    },
    {
      "epoch": 0.4608,
      "grad_norm": 0.44098421931266785,
      "learning_rate": 0.00017975394814234724,
      "loss": 0.6171,
      "step": 720
    },
    {
      "epoch": 0.4672,
      "grad_norm": 0.41493088006973267,
      "learning_rate": 0.00017912460615288382,
      "loss": 0.6553,
      "step": 730
    },
    {
      "epoch": 0.4736,
      "grad_norm": 0.5895330309867859,
      "learning_rate": 0.000178486769444224,
      "loss": 0.7354,
      "step": 740
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.6096502542495728,
      "learning_rate": 0.00017784050649372263,
      "loss": 0.7274,
      "step": 750
    },
    {
      "epoch": 0.4864,
      "grad_norm": 0.48541343212127686,
      "learning_rate": 0.0001771858866833654,
      "loss": 0.6771,
      "step": 760
    },
    {
      "epoch": 0.4928,
      "grad_norm": 0.4497716426849365,
      "learning_rate": 0.0001765229802923201,
      "loss": 0.6847,
      "step": 770
    },
    {
      "epoch": 0.4992,
      "grad_norm": 0.8974742293357849,
      "learning_rate": 0.00017585185848939143,
      "loss": 0.6835,
      "step": 780
    },
    {
      "epoch": 0.5056,
      "grad_norm": 0.551491916179657,
      "learning_rate": 0.00017517259332538055,
      "loss": 0.6781,
      "step": 790
    },
    {
      "epoch": 0.512,
      "grad_norm": 0.38401931524276733,
      "learning_rate": 0.00017448525772534968,
      "loss": 0.6966,
      "step": 800
    },
    {
      "epoch": 0.5184,
      "grad_norm": 0.4466804563999176,
      "learning_rate": 0.000173789925480793,
      "loss": 0.6893,
      "step": 810
    },
    {
      "epoch": 0.5248,
      "grad_norm": 0.5124428272247314,
      "learning_rate": 0.00017308667124171453,
      "loss": 0.6577,
      "step": 820
    },
    {
      "epoch": 0.5312,
      "grad_norm": 0.4320811331272125,
      "learning_rate": 0.00017237557050861374,
      "loss": 0.6906,
      "step": 830
    },
    {
      "epoch": 0.5376,
      "grad_norm": 0.4709755480289459,
      "learning_rate": 0.00017165669962437993,
      "loss": 0.7142,
      "step": 840
    },
    {
      "epoch": 0.544,
      "grad_norm": 0.3482053279876709,
      "learning_rate": 0.00017093013576609614,
      "loss": 0.6203,
      "step": 850
    },
    {
      "epoch": 0.5504,
      "grad_norm": 0.5808187127113342,
      "learning_rate": 0.0001701959569367535,
      "loss": 0.6553,
      "step": 860
    },
    {
      "epoch": 0.5568,
      "grad_norm": 0.6533963084220886,
      "learning_rate": 0.000169454241956877,
      "loss": 0.7042,
      "step": 870
    },
    {
      "epoch": 0.5632,
      "grad_norm": 0.44806984066963196,
      "learning_rate": 0.0001687050704560634,
      "loss": 0.691,
      "step": 880
    },
    {
      "epoch": 0.5696,
      "grad_norm": 0.48695850372314453,
      "learning_rate": 0.00016794852286443216,
      "loss": 0.6677,
      "step": 890
    },
    {
      "epoch": 0.576,
      "grad_norm": 0.37967655062675476,
      "learning_rate": 0.0001671846804039907,
      "loss": 0.5868,
      "step": 900
    },
    {
      "epoch": 0.5824,
      "grad_norm": 0.439632385969162,
      "learning_rate": 0.0001664136250799146,
      "loss": 0.6585,
      "step": 910
    },
    {
      "epoch": 0.5888,
      "grad_norm": 0.42822760343551636,
      "learning_rate": 0.00016563543967174326,
      "loss": 0.6614,
      "step": 920
    },
    {
      "epoch": 0.5952,
      "grad_norm": 0.7195959687232971,
      "learning_rate": 0.00016485020772449323,
      "loss": 0.7221,
      "step": 930
    },
    {
      "epoch": 0.6016,
      "grad_norm": 0.5174959301948547,
      "learning_rate": 0.00016405801353968865,
      "loss": 0.7228,
      "step": 940
    },
    {
      "epoch": 0.608,
      "grad_norm": 0.446869820356369,
      "learning_rate": 0.0001632589421663108,
      "loss": 0.6848,
      "step": 950
    },
    {
      "epoch": 0.6144,
      "grad_norm": 0.44321513175964355,
      "learning_rate": 0.00016245307939166735,
      "loss": 0.6435,
      "step": 960
    },
    {
      "epoch": 0.6208,
      "grad_norm": 0.38649019598960876,
      "learning_rate": 0.00016164051173218248,
      "loss": 0.6807,
      "step": 970
    },
    {
      "epoch": 0.6272,
      "grad_norm": 0.5043073892593384,
      "learning_rate": 0.0001608213264241083,
      "loss": 0.6459,
      "step": 980
    },
    {
      "epoch": 0.6336,
      "grad_norm": 0.5715827345848083,
      "learning_rate": 0.0001599956114141595,
      "loss": 0.5789,
      "step": 990
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.5520870089530945,
      "learning_rate": 0.0001591634553500714,
      "loss": 0.641,
      "step": 1000
    },
    {
      "epoch": 0.6464,
      "grad_norm": 0.4711968004703522,
      "learning_rate": 0.00015832494757108296,
      "loss": 0.6582,
      "step": 1010
    },
    {
      "epoch": 0.6528,
      "grad_norm": 0.3351499140262604,
      "learning_rate": 0.00015748017809834522,
      "loss": 0.6707,
      "step": 1020
    },
    {
      "epoch": 0.6592,
      "grad_norm": 0.5205023288726807,
      "learning_rate": 0.00015662923762525702,
      "loss": 0.7251,
      "step": 1030
    },
    {
      "epoch": 0.6656,
      "grad_norm": 0.45798584818840027,
      "learning_rate": 0.0001557722175077279,
      "loss": 0.7636,
      "step": 1040
    },
    {
      "epoch": 0.672,
      "grad_norm": 0.5280495285987854,
      "learning_rate": 0.0001549092097543707,
      "loss": 0.6573,
      "step": 1050
    },
    {
      "epoch": 0.6784,
      "grad_norm": 0.42536985874176025,
      "learning_rate": 0.00015404030701662318,
      "loss": 0.6923,
      "step": 1060
    },
    {
      "epoch": 0.6848,
      "grad_norm": 0.4808288514614105,
      "learning_rate": 0.0001531656025788014,
      "loss": 0.7328,
      "step": 1070
    },
    {
      "epoch": 0.6912,
      "grad_norm": 0.4689086675643921,
      "learning_rate": 0.00015228519034808468,
      "loss": 0.6759,
      "step": 1080
    },
    {
      "epoch": 0.6976,
      "grad_norm": 0.5867779850959778,
      "learning_rate": 0.0001513991648444339,
      "loss": 0.6597,
      "step": 1090
    },
    {
      "epoch": 0.704,
      "grad_norm": 0.4197504222393036,
      "learning_rate": 0.0001505076211904439,
      "loss": 0.6587,
      "step": 1100
    },
    {
      "epoch": 0.7104,
      "grad_norm": 0.43779894709587097,
      "learning_rate": 0.00014961065510113127,
      "loss": 0.667,
      "step": 1110
    },
    {
      "epoch": 0.7168,
      "grad_norm": 0.44096454977989197,
      "learning_rate": 0.0001487083628736586,
      "loss": 0.6597,
      "step": 1120
    },
    {
      "epoch": 0.7232,
      "grad_norm": 0.5331990122795105,
      "learning_rate": 0.00014780084137699587,
      "loss": 0.6938,
      "step": 1130
    },
    {
      "epoch": 0.7296,
      "grad_norm": 0.44433918595314026,
      "learning_rate": 0.00014688818804152104,
      "loss": 0.6399,
      "step": 1140
    },
    {
      "epoch": 0.736,
      "grad_norm": 0.5445188283920288,
      "learning_rate": 0.00014597050084855985,
      "loss": 0.6616,
      "step": 1150
    },
    {
      "epoch": 0.7424,
      "grad_norm": 0.6366716027259827,
      "learning_rate": 0.00014504787831986663,
      "loss": 0.7054,
      "step": 1160
    },
    {
      "epoch": 0.7488,
      "grad_norm": 0.5229464173316956,
      "learning_rate": 0.00014412041950704734,
      "loss": 0.6548,
      "step": 1170
    },
    {
      "epoch": 0.7552,
      "grad_norm": 0.5000469088554382,
      "learning_rate": 0.0001431882239809253,
      "loss": 0.6717,
      "step": 1180
    },
    {
      "epoch": 0.7616,
      "grad_norm": 0.7382975816726685,
      "learning_rate": 0.00014225139182085146,
      "loss": 0.6903,
      "step": 1190
    },
    {
      "epoch": 0.768,
      "grad_norm": 0.4773321747779846,
      "learning_rate": 0.00014131002360396008,
      "loss": 0.6697,
      "step": 1200
    },
    {
      "epoch": 0.7744,
      "grad_norm": 0.5243446230888367,
      "learning_rate": 0.00014036422039437083,
      "loss": 0.706,
      "step": 1210
    },
    {
      "epoch": 0.7808,
      "grad_norm": 0.3802701532840729,
      "learning_rate": 0.00013941408373233859,
      "loss": 0.63,
      "step": 1220
    },
    {
      "epoch": 0.7872,
      "grad_norm": 0.407234787940979,
      "learning_rate": 0.00013845971562335242,
      "loss": 0.6199,
      "step": 1230
    },
    {
      "epoch": 0.7936,
      "grad_norm": 0.4986175000667572,
      "learning_rate": 0.00013750121852718414,
      "loss": 0.6796,
      "step": 1240
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.5334459543228149,
      "learning_rate": 0.00013653869534688856,
      "loss": 0.6402,
      "step": 1250
    },
    {
      "epoch": 0.8064,
      "grad_norm": 0.48949408531188965,
      "learning_rate": 0.00013557224941775583,
      "loss": 0.6334,
      "step": 1260
    },
    {
      "epoch": 0.8128,
      "grad_norm": 0.39123228192329407,
      "learning_rate": 0.00013460198449621763,
      "loss": 0.691,
      "step": 1270
    },
    {
      "epoch": 0.8192,
      "grad_norm": 0.43434762954711914,
      "learning_rate": 0.00013362800474870772,
      "loss": 0.6919,
      "step": 1280
    },
    {
      "epoch": 0.8256,
      "grad_norm": 0.5047886967658997,
      "learning_rate": 0.00013265041474047915,
      "loss": 0.6796,
      "step": 1290
    },
    {
      "epoch": 0.832,
      "grad_norm": 0.5427054166793823,
      "learning_rate": 0.00013166931942437798,
      "loss": 0.6316,
      "step": 1300
    },
    {
      "epoch": 0.8384,
      "grad_norm": 0.6554842591285706,
      "learning_rate": 0.0001306848241295757,
      "loss": 0.7325,
      "step": 1310
    },
    {
      "epoch": 0.8448,
      "grad_norm": 0.43402552604675293,
      "learning_rate": 0.00012969703455026138,
      "loss": 0.6838,
      "step": 1320
    },
    {
      "epoch": 0.8512,
      "grad_norm": 0.5116223096847534,
      "learning_rate": 0.00012870605673429434,
      "loss": 0.696,
      "step": 1330
    },
    {
      "epoch": 0.8576,
      "grad_norm": 0.4341086149215698,
      "learning_rate": 0.000127711997071819,
      "loss": 0.641,
      "step": 1340
    },
    {
      "epoch": 0.864,
      "grad_norm": 0.44072291254997253,
      "learning_rate": 0.0001267149622838429,
      "loss": 0.674,
      "step": 1350
    },
    {
      "epoch": 0.8704,
      "grad_norm": 0.5048924088478088,
      "learning_rate": 0.0001257150594107796,
      "loss": 0.747,
      "step": 1360
    },
    {
      "epoch": 0.8768,
      "grad_norm": 0.6237353086471558,
      "learning_rate": 0.00012471239580095646,
      "loss": 0.6539,
      "step": 1370
    },
    {
      "epoch": 0.8832,
      "grad_norm": 0.4837043583393097,
      "learning_rate": 0.00012370707909909023,
      "loss": 0.6963,
      "step": 1380
    },
    {
      "epoch": 0.8896,
      "grad_norm": 0.5738978981971741,
      "learning_rate": 0.0001226992172347302,
      "loss": 0.6244,
      "step": 1390
    },
    {
      "epoch": 0.896,
      "grad_norm": 0.43260055780410767,
      "learning_rate": 0.00012168891841067129,
      "loss": 0.6186,
      "step": 1400
    },
    {
      "epoch": 0.9024,
      "grad_norm": 0.568681001663208,
      "learning_rate": 0.00012067629109133724,
      "loss": 0.6725,
      "step": 1410
    },
    {
      "epoch": 0.9088,
      "grad_norm": 0.44238707423210144,
      "learning_rate": 0.00011966144399113626,
      "loss": 0.6243,
      "step": 1420
    },
    {
      "epoch": 0.9152,
      "grad_norm": 0.4623650014400482,
      "learning_rate": 0.00011864448606278935,
      "loss": 0.7476,
      "step": 1430
    },
    {
      "epoch": 0.9216,
      "grad_norm": 0.3678796887397766,
      "learning_rate": 0.00011762552648563349,
      "loss": 0.6945,
      "step": 1440
    },
    {
      "epoch": 0.928,
      "grad_norm": 0.46330830454826355,
      "learning_rate": 0.00011660467465390019,
      "loss": 0.6665,
      "step": 1450
    },
    {
      "epoch": 0.9344,
      "grad_norm": 0.4844784736633301,
      "learning_rate": 0.00011558204016497094,
      "loss": 0.7025,
      "step": 1460
    },
    {
      "epoch": 0.9408,
      "grad_norm": 0.40738943219184875,
      "learning_rate": 0.00011455773280761122,
      "loss": 0.6759,
      "step": 1470
    },
    {
      "epoch": 0.9472,
      "grad_norm": 0.4546419680118561,
      "learning_rate": 0.00011353186255018351,
      "loss": 0.6661,
      "step": 1480
    },
    {
      "epoch": 0.9536,
      "grad_norm": 0.4600119888782501,
      "learning_rate": 0.00011250453952884126,
      "loss": 0.6618,
      "step": 1490
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.4683782756328583,
      "learning_rate": 0.00011147587403570496,
      "loss": 0.6787,
      "step": 1500
    },
    {
      "epoch": 0.9664,
      "grad_norm": 0.3876534700393677,
      "learning_rate": 0.00011044597650702111,
      "loss": 0.5883,
      "step": 1510
    },
    {
      "epoch": 0.9728,
      "grad_norm": 0.4285566806793213,
      "learning_rate": 0.00010941495751130602,
      "loss": 0.7137,
      "step": 1520
    },
    {
      "epoch": 0.9792,
      "grad_norm": 0.40321892499923706,
      "learning_rate": 0.00010838292773747531,
      "loss": 0.6683,
      "step": 1530
    },
    {
      "epoch": 0.9856,
      "grad_norm": 0.3609747588634491,
      "learning_rate": 0.00010734999798296057,
      "loss": 0.634,
      "step": 1540
    },
    {
      "epoch": 0.992,
      "grad_norm": 0.3602063059806824,
      "learning_rate": 0.00010631627914181404,
      "loss": 0.6399,
      "step": 1550
    },
    {
      "epoch": 0.9984,
      "grad_norm": 0.5612394213676453,
      "learning_rate": 0.0001052818821928034,
      "loss": 0.6318,
      "step": 1560
    },
    {
      "epoch": 1.00448,
      "grad_norm": 0.4141237735748291,
      "learning_rate": 0.00010424691818749716,
      "loss": 0.6482,
      "step": 1570
    },
    {
      "epoch": 1.01088,
      "grad_norm": 0.5634016394615173,
      "learning_rate": 0.00010321149823834226,
      "loss": 0.597,
      "step": 1580
    },
    {
      "epoch": 1.01728,
      "grad_norm": 0.5324051976203918,
      "learning_rate": 0.00010217573350673521,
      "loss": 0.608,
      "step": 1590
    },
    {
      "epoch": 1.02368,
      "grad_norm": 0.38127145171165466,
      "learning_rate": 0.00010113973519108798,
      "loss": 0.5887,
      "step": 1600
    },
    {
      "epoch": 1.03008,
      "grad_norm": 0.5892088413238525,
      "learning_rate": 0.0001001036145148897,
      "loss": 0.6591,
      "step": 1610
    },
    {
      "epoch": 1.03648,
      "grad_norm": 0.5768296718597412,
      "learning_rate": 9.906748271476614e-05,
      "loss": 0.6329,
      "step": 1620
    },
    {
      "epoch": 1.04288,
      "grad_norm": 0.4188311696052551,
      "learning_rate": 9.803145102853722e-05,
      "loss": 0.6158,
      "step": 1630
    },
    {
      "epoch": 1.04928,
      "grad_norm": 0.5551757216453552,
      "learning_rate": 9.699563068327479e-05,
      "loss": 0.6546,
      "step": 1640
    },
    {
      "epoch": 1.05568,
      "grad_norm": 0.5646746158599854,
      "learning_rate": 9.596013288336136e-05,
      "loss": 0.5468,
      "step": 1650
    },
    {
      "epoch": 1.06208,
      "grad_norm": 0.688260018825531,
      "learning_rate": 9.492506879855143e-05,
      "loss": 0.618,
      "step": 1660
    },
    {
      "epoch": 1.06848,
      "grad_norm": 0.7097967863082886,
      "learning_rate": 9.389054955203636e-05,
      "loss": 0.6043,
      "step": 1670
    },
    {
      "epoch": 1.07488,
      "grad_norm": 0.4617909789085388,
      "learning_rate": 9.285668620851436e-05,
      "loss": 0.6102,
      "step": 1680
    },
    {
      "epoch": 1.08128,
      "grad_norm": 0.4918585419654846,
      "learning_rate": 9.182358976226666e-05,
      "loss": 0.5913,
      "step": 1690
    },
    {
      "epoch": 1.08768,
      "grad_norm": 0.40430760383605957,
      "learning_rate": 9.079137112524141e-05,
      "loss": 0.5601,
      "step": 1700
    },
    {
      "epoch": 1.09408,
      "grad_norm": 0.5571460127830505,
      "learning_rate": 8.976014111514623e-05,
      "loss": 0.6299,
      "step": 1710
    },
    {
      "epoch": 1.10048,
      "grad_norm": 0.39700648188591003,
      "learning_rate": 8.873001044355103e-05,
      "loss": 0.6348,
      "step": 1720
    },
    {
      "epoch": 1.10688,
      "grad_norm": 0.3656598925590515,
      "learning_rate": 8.770108970400206e-05,
      "loss": 0.5812,
      "step": 1730
    },
    {
      "epoch": 1.11328,
      "grad_norm": 0.43386149406433105,
      "learning_rate": 8.667348936014873e-05,
      "loss": 0.5755,
      "step": 1740
    },
    {
      "epoch": 1.11968,
      "grad_norm": 0.4939075708389282,
      "learning_rate": 8.564731973388448e-05,
      "loss": 0.6054,
      "step": 1750
    },
    {
      "epoch": 1.12608,
      "grad_norm": 0.5425501465797424,
      "learning_rate": 8.462269099350272e-05,
      "loss": 0.6171,
      "step": 1760
    },
    {
      "epoch": 1.13248,
      "grad_norm": 0.5405861139297485,
      "learning_rate": 8.359971314186919e-05,
      "loss": 0.6003,
      "step": 1770
    },
    {
      "epoch": 1.13888,
      "grad_norm": 0.6686437726020813,
      "learning_rate": 8.25784960046123e-05,
      "loss": 0.6332,
      "step": 1780
    },
    {
      "epoch": 1.14528,
      "grad_norm": 0.5693305134773254,
      "learning_rate": 8.155914921833242e-05,
      "loss": 0.6056,
      "step": 1790
    },
    {
      "epoch": 1.15168,
      "grad_norm": 0.4096905291080475,
      "learning_rate": 8.054178221883121e-05,
      "loss": 0.6108,
      "step": 1800
    },
    {
      "epoch": 1.15808,
      "grad_norm": 0.5007911324501038,
      "learning_rate": 7.952650422936303e-05,
      "loss": 0.5853,
      "step": 1810
    },
    {
      "epoch": 1.16448,
      "grad_norm": 0.412009060382843,
      "learning_rate": 7.851342424890868e-05,
      "loss": 0.5415,
      "step": 1820
    },
    {
      "epoch": 1.17088,
      "grad_norm": 0.5231996774673462,
      "learning_rate": 7.750265104047348e-05,
      "loss": 0.6343,
      "step": 1830
    },
    {
      "epoch": 1.17728,
      "grad_norm": 0.4968413710594177,
      "learning_rate": 7.649429311941052e-05,
      "loss": 0.6104,
      "step": 1840
    },
    {
      "epoch": 1.18368,
      "grad_norm": 0.6191391348838806,
      "learning_rate": 7.548845874177066e-05,
      "loss": 0.5985,
      "step": 1850
    },
    {
      "epoch": 1.19008,
      "grad_norm": 0.5319228172302246,
      "learning_rate": 7.44852558926803e-05,
      "loss": 0.5605,
      "step": 1860
    },
    {
      "epoch": 1.19648,
      "grad_norm": 0.535172700881958,
      "learning_rate": 7.348479227474821e-05,
      "loss": 0.6112,
      "step": 1870
    },
    {
      "epoch": 1.20288,
      "grad_norm": 0.48331406712532043,
      "learning_rate": 7.248717529650271e-05,
      "loss": 0.6116,
      "step": 1880
    },
    {
      "epoch": 1.20928,
      "grad_norm": 0.5666651129722595,
      "learning_rate": 7.14925120608604e-05,
      "loss": 0.6305,
      "step": 1890
    },
    {
      "epoch": 1.21568,
      "grad_norm": 0.4666951298713684,
      "learning_rate": 7.050090935362779e-05,
      "loss": 0.553,
      "step": 1900
    },
    {
      "epoch": 1.22208,
      "grad_norm": 0.5161206722259521,
      "learning_rate": 6.95124736320369e-05,
      "loss": 0.6262,
      "step": 1910
    },
    {
      "epoch": 1.22848,
      "grad_norm": 0.531605064868927,
      "learning_rate": 6.8527311013316e-05,
      "loss": 0.6618,
      "step": 1920
    },
    {
      "epoch": 1.23488,
      "grad_norm": 0.6067713499069214,
      "learning_rate": 6.754552726329721e-05,
      "loss": 0.6051,
      "step": 1930
    },
    {
      "epoch": 1.24128,
      "grad_norm": 0.5368122458457947,
      "learning_rate": 6.65672277850615e-05,
      "loss": 0.6113,
      "step": 1940
    },
    {
      "epoch": 1.24768,
      "grad_norm": 1.011513113975525,
      "learning_rate": 6.559251760762276e-05,
      "loss": 0.605,
      "step": 1950
    },
    {
      "epoch": 1.25408,
      "grad_norm": 0.6229475140571594,
      "learning_rate": 6.462150137465194e-05,
      "loss": 0.6122,
      "step": 1960
    },
    {
      "epoch": 1.26048,
      "grad_norm": 0.459082692861557,
      "learning_rate": 6.365428333324288e-05,
      "loss": 0.6376,
      "step": 1970
    },
    {
      "epoch": 1.26688,
      "grad_norm": 0.5261330008506775,
      "learning_rate": 6.269096732272008e-05,
      "loss": 0.644,
      "step": 1980
    },
    {
      "epoch": 1.27328,
      "grad_norm": 0.4571448564529419,
      "learning_rate": 6.173165676349103e-05,
      "loss": 0.606,
      "step": 1990
    },
    {
      "epoch": 1.27968,
      "grad_norm": 0.5098862648010254,
      "learning_rate": 6.0776454645942905e-05,
      "loss": 0.5959,
      "step": 2000
    },
    {
      "epoch": 1.2860800000000001,
      "grad_norm": 0.5272312164306641,
      "learning_rate": 5.982546351938571e-05,
      "loss": 0.5994,
      "step": 2010
    },
    {
      "epoch": 1.29248,
      "grad_norm": 0.6578050851821899,
      "learning_rate": 5.887878548104271e-05,
      "loss": 0.5812,
      "step": 2020
    },
    {
      "epoch": 1.29888,
      "grad_norm": 0.5783313512802124,
      "learning_rate": 5.793652216508929e-05,
      "loss": 0.5423,
      "step": 2030
    },
    {
      "epoch": 1.30528,
      "grad_norm": 0.4907006323337555,
      "learning_rate": 5.6998774731741845e-05,
      "loss": 0.5797,
      "step": 2040
    },
    {
      "epoch": 1.31168,
      "grad_norm": 0.5170001983642578,
      "learning_rate": 5.6065643856397234e-05,
      "loss": 0.5233,
      "step": 2050
    },
    {
      "epoch": 1.31808,
      "grad_norm": 0.6264833211898804,
      "learning_rate": 5.513722971882434e-05,
      "loss": 0.5853,
      "step": 2060
    },
    {
      "epoch": 1.3244799999999999,
      "grad_norm": 0.5448036193847656,
      "learning_rate": 5.421363199240898e-05,
      "loss": 0.6044,
      "step": 2070
    },
    {
      "epoch": 1.33088,
      "grad_norm": 0.550881564617157,
      "learning_rate": 5.329494983345311e-05,
      "loss": 0.5785,
      "step": 2080
    },
    {
      "epoch": 1.33728,
      "grad_norm": 0.633694052696228,
      "learning_rate": 5.238128187052938e-05,
      "loss": 0.585,
      "step": 2090
    },
    {
      "epoch": 1.34368,
      "grad_norm": 0.4956178665161133,
      "learning_rate": 5.1472726193892676e-05,
      "loss": 0.5458,
      "step": 2100
    },
    {
      "epoch": 1.35008,
      "grad_norm": 0.3727697432041168,
      "learning_rate": 5.056938034494917e-05,
      "loss": 0.5804,
      "step": 2110
    },
    {
      "epoch": 1.35648,
      "grad_norm": 0.6883597373962402,
      "learning_rate": 4.9671341305784436e-05,
      "loss": 0.6611,
      "step": 2120
    },
    {
      "epoch": 1.36288,
      "grad_norm": 0.6348884105682373,
      "learning_rate": 4.8778705488751544e-05,
      "loss": 0.6036,
      "step": 2130
    },
    {
      "epoch": 1.36928,
      "grad_norm": 0.5911450386047363,
      "learning_rate": 4.7891568726120397e-05,
      "loss": 0.6182,
      "step": 2140
    },
    {
      "epoch": 1.37568,
      "grad_norm": 0.5730741620063782,
      "learning_rate": 4.7010026259789176e-05,
      "loss": 0.6174,
      "step": 2150
    },
    {
      "epoch": 1.38208,
      "grad_norm": 0.5857165455818176,
      "learning_rate": 4.6134172731059445e-05,
      "loss": 0.6127,
      "step": 2160
    },
    {
      "epoch": 1.38848,
      "grad_norm": 0.5052241683006287,
      "learning_rate": 4.526410217047552e-05,
      "loss": 0.6309,
      "step": 2170
    },
    {
      "epoch": 1.3948800000000001,
      "grad_norm": 0.5409134030342102,
      "learning_rate": 4.439990798772943e-05,
      "loss": 0.6291,
      "step": 2180
    },
    {
      "epoch": 1.40128,
      "grad_norm": 0.5601212978363037,
      "learning_rate": 4.354168296163264e-05,
      "loss": 0.6101,
      "step": 2190
    },
    {
      "epoch": 1.40768,
      "grad_norm": 0.5106992125511169,
      "learning_rate": 4.2689519230155274e-05,
      "loss": 0.5298,
      "step": 2200
    },
    {
      "epoch": 1.41408,
      "grad_norm": 0.49956420063972473,
      "learning_rate": 4.1843508280534536e-05,
      "loss": 0.6648,
      "step": 2210
    },
    {
      "epoch": 1.42048,
      "grad_norm": 0.45011088252067566,
      "learning_rate": 4.100374093945258e-05,
      "loss": 0.5841,
      "step": 2220
    },
    {
      "epoch": 1.42688,
      "grad_norm": 0.4899084270000458,
      "learning_rate": 4.017030736328558e-05,
      "loss": 0.5929,
      "step": 2230
    },
    {
      "epoch": 1.4332799999999999,
      "grad_norm": 0.5421857833862305,
      "learning_rate": 3.934329702842444e-05,
      "loss": 0.6157,
      "step": 2240
    },
    {
      "epoch": 1.43968,
      "grad_norm": 0.5228779315948486,
      "learning_rate": 3.852279872166904e-05,
      "loss": 0.6651,
      "step": 2250
    },
    {
      "epoch": 1.44608,
      "grad_norm": 0.5113914608955383,
      "learning_rate": 3.7708900530695965e-05,
      "loss": 0.5452,
      "step": 2260
    },
    {
      "epoch": 1.45248,
      "grad_norm": 0.5639584064483643,
      "learning_rate": 3.6901689834601617e-05,
      "loss": 0.5906,
      "step": 2270
    },
    {
      "epoch": 1.45888,
      "grad_norm": 0.6071643233299255,
      "learning_rate": 3.6101253294521187e-05,
      "loss": 0.5745,
      "step": 2280
    },
    {
      "epoch": 1.46528,
      "grad_norm": 0.6311919093132019,
      "learning_rate": 3.5307676844325e-05,
      "loss": 0.5739,
      "step": 2290
    },
    {
      "epoch": 1.47168,
      "grad_norm": 0.565895140171051,
      "learning_rate": 3.452104568139265e-05,
      "loss": 0.5769,
      "step": 2300
    },
    {
      "epoch": 1.47808,
      "grad_norm": 0.4229973554611206,
      "learning_rate": 3.374144425746634e-05,
      "loss": 0.564,
      "step": 2310
    },
    {
      "epoch": 1.48448,
      "grad_norm": 0.8774714469909668,
      "learning_rate": 3.296895626958425e-05,
      "loss": 0.5763,
      "step": 2320
    },
    {
      "epoch": 1.49088,
      "grad_norm": 0.5310688018798828,
      "learning_rate": 3.220366465109488e-05,
      "loss": 0.5822,
      "step": 2330
    },
    {
      "epoch": 1.49728,
      "grad_norm": 0.47269049286842346,
      "learning_rate": 3.144565156275352e-05,
      "loss": 0.6132,
      "step": 2340
    },
    {
      "epoch": 1.5036800000000001,
      "grad_norm": 0.417751282453537,
      "learning_rate": 3.0694998383901506e-05,
      "loss": 0.5922,
      "step": 2350
    },
    {
      "epoch": 1.5100799999999999,
      "grad_norm": 0.6422446966171265,
      "learning_rate": 2.9951785703729483e-05,
      "loss": 0.5811,
      "step": 2360
    },
    {
      "epoch": 1.51648,
      "grad_norm": 0.4363415837287903,
      "learning_rate": 2.921609331262538e-05,
      "loss": 0.5312,
      "step": 2370
    },
    {
      "epoch": 1.52288,
      "grad_norm": 0.493112713098526,
      "learning_rate": 2.8488000193608312e-05,
      "loss": 0.5452,
      "step": 2380
    },
    {
      "epoch": 1.52928,
      "grad_norm": 0.7167499661445618,
      "learning_rate": 2.776758451384891e-05,
      "loss": 0.5449,
      "step": 2390
    },
    {
      "epoch": 1.5356800000000002,
      "grad_norm": 0.6153023838996887,
      "learning_rate": 2.705492361627757e-05,
      "loss": 0.6211,
      "step": 2400
    },
    {
      "epoch": 1.54208,
      "grad_norm": 0.5327563285827637,
      "learning_rate": 2.63500940112809e-05,
      "loss": 0.6054,
      "step": 2410
    },
    {
      "epoch": 1.54848,
      "grad_norm": 0.5175975561141968,
      "learning_rate": 2.5653171368487705e-05,
      "loss": 0.5633,
      "step": 2420
    },
    {
      "epoch": 1.55488,
      "grad_norm": 0.6553635001182556,
      "learning_rate": 2.4964230508645148e-05,
      "loss": 0.5492,
      "step": 2430
    },
    {
      "epoch": 1.56128,
      "grad_norm": 0.5845716595649719,
      "learning_rate": 2.4283345395586143e-05,
      "loss": 0.5536,
      "step": 2440
    },
    {
      "epoch": 1.56768,
      "grad_norm": 0.5456920266151428,
      "learning_rate": 2.3610589128288595e-05,
      "loss": 0.5963,
      "step": 2450
    },
    {
      "epoch": 1.57408,
      "grad_norm": 0.5026968717575073,
      "learning_rate": 2.294603393302772e-05,
      "loss": 0.5714,
      "step": 2460
    },
    {
      "epoch": 1.58048,
      "grad_norm": 0.5262892842292786,
      "learning_rate": 2.228975115562182e-05,
      "loss": 0.6078,
      "step": 2470
    },
    {
      "epoch": 1.5868799999999998,
      "grad_norm": 0.5293163061141968,
      "learning_rate": 2.1641811253772725e-05,
      "loss": 0.6253,
      "step": 2480
    },
    {
      "epoch": 1.59328,
      "grad_norm": 0.6409740447998047,
      "learning_rate": 2.1002283789501542e-05,
      "loss": 0.6156,
      "step": 2490
    },
    {
      "epoch": 1.59968,
      "grad_norm": 0.6660425662994385,
      "learning_rate": 2.0371237421680544e-05,
      "loss": 0.6256,
      "step": 2500
    },
    {
      "epoch": 1.60608,
      "grad_norm": 0.6069573163986206,
      "learning_rate": 1.9748739898662017e-05,
      "loss": 0.5713,
      "step": 2510
    },
    {
      "epoch": 1.6124800000000001,
      "grad_norm": 0.522100567817688,
      "learning_rate": 1.9134858051004957e-05,
      "loss": 0.5491,
      "step": 2520
    },
    {
      "epoch": 1.6188799999999999,
      "grad_norm": 0.5673690438270569,
      "learning_rate": 1.8529657784300168e-05,
      "loss": 0.5975,
      "step": 2530
    },
    {
      "epoch": 1.62528,
      "grad_norm": 0.5296013355255127,
      "learning_rate": 1.7933204072094732e-05,
      "loss": 0.6127,
      "step": 2540
    },
    {
      "epoch": 1.63168,
      "grad_norm": 0.5688774585723877,
      "learning_rate": 1.734556094891655e-05,
      "loss": 0.6134,
      "step": 2550
    },
    {
      "epoch": 1.63808,
      "grad_norm": 0.6334071159362793,
      "learning_rate": 1.6766791503399638e-05,
      "loss": 0.6308,
      "step": 2560
    },
    {
      "epoch": 1.6444800000000002,
      "grad_norm": 0.4486570656299591,
      "learning_rate": 1.6196957871510986e-05,
      "loss": 0.6144,
      "step": 2570
    },
    {
      "epoch": 1.65088,
      "grad_norm": 0.5142876505851746,
      "learning_rate": 1.5636121229879817e-05,
      "loss": 0.5874,
      "step": 2580
    },
    {
      "epoch": 1.65728,
      "grad_norm": 0.4732832610607147,
      "learning_rate": 1.5084341789229672e-05,
      "loss": 0.6272,
      "step": 2590
    },
    {
      "epoch": 1.66368,
      "grad_norm": 0.5612172484397888,
      "learning_rate": 1.4541678787914259e-05,
      "loss": 0.6462,
      "step": 2600
    },
    {
      "epoch": 1.67008,
      "grad_norm": 0.43661874532699585,
      "learning_rate": 1.4008190485557704e-05,
      "loss": 0.5975,
      "step": 2610
    },
    {
      "epoch": 1.67648,
      "grad_norm": 0.6018132567405701,
      "learning_rate": 1.3483934156799938e-05,
      "loss": 0.575,
      "step": 2620
    },
    {
      "epoch": 1.68288,
      "grad_norm": 0.5026175379753113,
      "learning_rate": 1.2968966085147571e-05,
      "loss": 0.596,
      "step": 2630
    },
    {
      "epoch": 1.6892800000000001,
      "grad_norm": 0.5254365801811218,
      "learning_rate": 1.2463341556931629e-05,
      "loss": 0.6467,
      "step": 2640
    },
    {
      "epoch": 1.6956799999999999,
      "grad_norm": 0.48261141777038574,
      "learning_rate": 1.1967114855371874e-05,
      "loss": 0.5596,
      "step": 2650
    },
    {
      "epoch": 1.70208,
      "grad_norm": 0.5591092705726624,
      "learning_rate": 1.1480339254749129e-05,
      "loss": 0.6207,
      "step": 2660
    },
    {
      "epoch": 1.70848,
      "grad_norm": 0.515070378780365,
      "learning_rate": 1.1003067014685797e-05,
      "loss": 0.6142,
      "step": 2670
    },
    {
      "epoch": 1.71488,
      "grad_norm": 0.5016090273857117,
      "learning_rate": 1.0535349374535263e-05,
      "loss": 0.6116,
      "step": 2680
    },
    {
      "epoch": 1.7212800000000001,
      "grad_norm": 0.5215820670127869,
      "learning_rate": 1.007723654788103e-05,
      "loss": 0.6188,
      "step": 2690
    },
    {
      "epoch": 1.7276799999999999,
      "grad_norm": 0.471014142036438,
      "learning_rate": 9.628777717145765e-06,
      "loss": 0.6329,
      "step": 2700
    },
    {
      "epoch": 1.73408,
      "grad_norm": 0.38443946838378906,
      "learning_rate": 9.190021028311157e-06,
      "loss": 0.6165,
      "step": 2710
    },
    {
      "epoch": 1.74048,
      "grad_norm": 0.5631875395774841,
      "learning_rate": 8.761013585748979e-06,
      "loss": 0.6077,
      "step": 2720
    },
    {
      "epoch": 1.74688,
      "grad_norm": 0.6198865175247192,
      "learning_rate": 8.34180144716411e-06,
      "loss": 0.5734,
      "step": 2730
    },
    {
      "epoch": 1.75328,
      "grad_norm": 0.40406545996665955,
      "learning_rate": 7.932429618649795e-06,
      "loss": 0.5933,
      "step": 2740
    },
    {
      "epoch": 1.75968,
      "grad_norm": 0.5280314683914185,
      "learning_rate": 7.532942049855784e-06,
      "loss": 0.6117,
      "step": 2750
    },
    {
      "epoch": 1.76608,
      "grad_norm": 0.5172168016433716,
      "learning_rate": 7.143381629270074e-06,
      "loss": 0.6114,
      "step": 2760
    },
    {
      "epoch": 1.77248,
      "grad_norm": 0.5068448185920715,
      "learning_rate": 6.763790179614371e-06,
      "loss": 0.5435,
      "step": 2770
    },
    {
      "epoch": 1.77888,
      "grad_norm": 0.5440196394920349,
      "learning_rate": 6.394208453354067e-06,
      "loss": 0.521,
      "step": 2780
    },
    {
      "epoch": 1.78528,
      "grad_norm": 0.5539653897285461,
      "learning_rate": 6.034676128323124e-06,
      "loss": 0.6148,
      "step": 2790
    },
    {
      "epoch": 1.79168,
      "grad_norm": 0.4259902834892273,
      "learning_rate": 5.6852318034642745e-06,
      "loss": 0.6014,
      "step": 2800
    },
    {
      "epoch": 1.7980800000000001,
      "grad_norm": 0.6485546231269836,
      "learning_rate": 5.345912994685142e-06,
      "loss": 0.6171,
      "step": 2810
    },
    {
      "epoch": 1.8044799999999999,
      "grad_norm": 0.6626587510108948,
      "learning_rate": 5.016756130830547e-06,
      "loss": 0.5744,
      "step": 2820
    },
    {
      "epoch": 1.81088,
      "grad_norm": 0.46440181136131287,
      "learning_rate": 4.697796549771538e-06,
      "loss": 0.5398,
      "step": 2830
    },
    {
      "epoch": 1.81728,
      "grad_norm": 0.5672409534454346,
      "learning_rate": 4.389068494611615e-06,
      "loss": 0.5537,
      "step": 2840
    },
    {
      "epoch": 1.82368,
      "grad_norm": 0.61570143699646,
      "learning_rate": 4.09060511001036e-06,
      "loss": 0.611,
      "step": 2850
    },
    {
      "epoch": 1.8300800000000002,
      "grad_norm": 0.5929261445999146,
      "learning_rate": 3.8024384386251223e-06,
      "loss": 0.6261,
      "step": 2860
    },
    {
      "epoch": 1.83648,
      "grad_norm": 0.5820772051811218,
      "learning_rate": 3.5245994176709263e-06,
      "loss": 0.5584,
      "step": 2870
    },
    {
      "epoch": 1.84288,
      "grad_norm": 0.49921396374702454,
      "learning_rate": 3.257117875599125e-06,
      "loss": 0.5457,
      "step": 2880
    },
    {
      "epoch": 1.84928,
      "grad_norm": 0.5223290920257568,
      "learning_rate": 3.00002252889503e-06,
      "loss": 0.6124,
      "step": 2890
    },
    {
      "epoch": 1.85568,
      "grad_norm": 0.5064321756362915,
      "learning_rate": 2.753340978994945e-06,
      "loss": 0.5743,
      "step": 2900
    },
    {
      "epoch": 1.86208,
      "grad_norm": 0.6211758852005005,
      "learning_rate": 2.517099709322923e-06,
      "loss": 0.5903,
      "step": 2910
    },
    {
      "epoch": 1.86848,
      "grad_norm": 0.687255322933197,
      "learning_rate": 2.2913240824475103e-06,
      "loss": 0.5641,
      "step": 2920
    },
    {
      "epoch": 1.87488,
      "grad_norm": 0.6268757581710815,
      "learning_rate": 2.07603833735891e-06,
      "loss": 0.5876,
      "step": 2930
    },
    {
      "epoch": 1.8812799999999998,
      "grad_norm": 0.4887927174568176,
      "learning_rate": 1.871265586866633e-06,
      "loss": 0.5799,
      "step": 2940
    },
    {
      "epoch": 1.88768,
      "grad_norm": 0.7193939685821533,
      "learning_rate": 1.677027815118215e-06,
      "loss": 0.5409,
      "step": 2950
    },
    {
      "epoch": 1.89408,
      "grad_norm": 0.6614452600479126,
      "learning_rate": 1.493345875238994e-06,
      "loss": 0.5617,
      "step": 2960
    },
    {
      "epoch": 1.90048,
      "grad_norm": 0.4994400441646576,
      "learning_rate": 1.3202394870933111e-06,
      "loss": 0.5806,
      "step": 2970
    },
    {
      "epoch": 1.9068800000000001,
      "grad_norm": 0.5577387809753418,
      "learning_rate": 1.1577272351674607e-06,
      "loss": 0.5865,
      "step": 2980
    },
    {
      "epoch": 1.9132799999999999,
      "grad_norm": 0.5367772579193115,
      "learning_rate": 1.0058265665744637e-06,
      "loss": 0.5826,
      "step": 2990
    },
    {
      "epoch": 1.91968,
      "grad_norm": 0.5801472067832947,
      "learning_rate": 8.645537891809663e-07,
      "loss": 0.5724,
      "step": 3000
    }
  ],
  "logging_steps": 10,
  "max_steps": 3126,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 200,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.1084714410927718e+18,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
